{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easyocr\n",
    "import os\n",
    "import torch\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "from mmocr_eval import eval_ocr_metric\n",
    "import re\n",
    "import cv2 as cv\n",
    "import onnxruntime\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install onnx\n",
    "# !pip install onnxruntime-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "model_path: trainer\\saved_models\\crnn_30k_lr_25e-2\\crnn_30k_lr_25e-2.pth\n",
      "config: my_model\\user_network\\crnn.yaml\n",
      "--------------------\n",
      "Transformation: None\n",
      "FeatureExtraction: VGG\n",
      "SequenceModeling: BiLSTM\n",
      "Prediction: CTC\n"
     ]
    }
   ],
   "source": [
    "model = 'crnn_30k_lr_25e-2' #folder name of model\n",
    "iteration = 'crnn_30k_lr_25e-2' # model .pth file name\n",
    "\n",
    "model_path = os.path.join('trainer', 'saved_models',model)\n",
    "reader = easyocr.Reader(lang_list = ['en','th'],\n",
    "                    model_storage_directory = model_path,\n",
    "                    user_network_directory = os.path.join('my_model','user_network'),\n",
    "                    recog_network  = iteration,\n",
    "                    config_path = 'crnn', #configuration file name use to get character_list only  (so able to use for every model)\n",
    "                    gpu = True)\n",
    "# reader = easyocr.Reader(lang_list = ['en','th'],gpu = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([[0, 0], [103, 0], [103, 59], [0, 59]], 'วอห์น', 0.5715873732040726)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepaths = glob.glob(os.path.join('trainer/all_data/testing/testing','*.jpg'))\n",
    "filepaths = sorted(filepaths, key= lambda x: int(re.split(r'[/\\\\.]',x)[-2]))\n",
    "random_file_path = filepaths[0]\n",
    "\n",
    "img = plt.imread(random_file_path)\n",
    "# print (f'image befor reader: {img[0][0]}')\n",
    "result = reader.recognize(random_file_path)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Intern\\EasyOCR\\deploymeny.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Intern/EasyOCR/deploymeny.ipynb#ch0000005?line=100'>101</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/Intern/EasyOCR/deploymeny.ipynb#ch0000005?line=101'>102</a>\u001b[0m     pred\u001b[39m.\u001b[39;49mappend(result)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Intern/EasyOCR/deploymeny.ipynb#ch0000005?line=102'>103</a>\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'append'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Intern\\EasyOCR\\deploymeny.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Intern/EasyOCR/deploymeny.ipynb#ch0000005?line=101'>102</a>\u001b[0m         pred\u001b[39m.\u001b[39mappend(result)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Intern/EasyOCR/deploymeny.ipynb#ch0000005?line=102'>103</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/Intern/EasyOCR/deploymeny.ipynb#ch0000005?line=103'>104</a>\u001b[0m         pred\u001b[39m.\u001b[39;49mappend(\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Intern/EasyOCR/deploymeny.ipynb#ch0000005?line=105'>106</a>\u001b[0m label_img \u001b[39m=\u001b[39m label[i][\u001b[39m1\u001b[39m]\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Intern/EasyOCR/deploymeny.ipynb#ch0000005?line=106'>107</a>\u001b[0m name_img\u001b[39m.\u001b[39mappend(label[i][\u001b[39m0\u001b[39m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy()if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "def custom_mean(x):\n",
    "    return x.prod()**(2.0/np.sqrt(len(x)))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from easyocr.utils import reformat_input\n",
    "from easyocr.utils import get_image_list\n",
    "from easyocr.recognition import AlignCollate\n",
    "from easyocr.recognition import ListDataset\n",
    "import torch.nn.functional as F\n",
    "from easyocr.utils import CTCLabelConverter\n",
    "\n",
    "filepaths = glob.glob(os.path.join('trainer/all_data/testing/testing','*.jpg'))\n",
    "# filepaths = glob.glob(os.path.join('trainer/all_data/training/training','*.jpg'))[:1000]\n",
    "filepaths = sorted(filepaths, key= lambda x: int(re.split(r'[/\\\\.]',x)[-2]))\n",
    "label_path = 'trainer/all_data/testing/testing/label.txt'\n",
    "with open(label_path, encoding='utf8') as file:\n",
    "    label = np.loadtxt(file,dtype=str)\n",
    "start = time.time()\n",
    "filepaths = filepaths[:10]\n",
    "pred_list = []\n",
    "gt = [] # ground truth\n",
    "name_img = []\n",
    "\n",
    "for i_img in tqdm(range(len(filepaths))):\n",
    "    img, img_cv_grey = reformat_input(random_file_path)\n",
    "    # print(f'reformat {img_cv_grey}')\n",
    "    y_max, x_max = img_cv_grey.shape\n",
    "    horizontal_list = [[0, x_max, 0, y_max]]\n",
    "    for bbox in horizontal_list:\n",
    "                    h_list = [bbox]\n",
    "                    f_list = []\n",
    "                    image_list, max_width = get_image_list(h_list, f_list, img_cv_grey, model_height = 64)\n",
    "    # print(f'image_list: {image_list}')\n",
    "    coord = [item[0] for item in image_list]\n",
    "    img_list = [item[1] for item in image_list]\n",
    "    # print(f'image_list: {img_list}')\n",
    "    AlignCollate_normal = AlignCollate(imgH=64, imgW=600, keep_ratio_with_pad=True)\n",
    "    test_data = ListDataset(img_list)\n",
    "    # print(f'test_data {test_data}')\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "            test_data, batch_size=1, shuffle=False,\n",
    "            num_workers=int(1), collate_fn=AlignCollate_normal, pin_memory=True)\n",
    "    # print(f'test_loader {test_loader}')\n",
    "    ort_session = onnxruntime.InferenceSession(\"recognitionModel.onnx\", providers=['CUDAExecutionProvider', 'TensorrtExecutionProvider'])\n",
    "    ort_session.set_providers(['CUDAExecutionProvider', 'TensorrtExecutionProvider'])\n",
    "    for image_tensors in test_loader:\n",
    "        image = image_tensors.to('cuda')\n",
    "        # print(f'image_tensors {to_numpy(image)}')\n",
    "        start = time.time()\n",
    "        ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(image)}\n",
    "\n",
    "        ort_outs = ort_session.run(None, ort_inputs)\n",
    "        preds = torch.from_numpy(ort_outs[0])\n",
    "        device = torch.device(\"cuda\")\n",
    "        batch_size = 1\n",
    "        ignore_idx = []\n",
    "        preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
    "        preds_prob = F.softmax(preds, dim=2)\n",
    "        preds_prob = preds_prob.cpu().detach().numpy()\n",
    "        preds_prob[:,:,ignore_idx] = 0.\n",
    "        pred_norm = preds_prob.sum(axis=2)\n",
    "        preds_prob = preds_prob/np.expand_dims(pred_norm, axis=-1)\n",
    "        preds_prob = torch.from_numpy(preds_prob).float().to(device)\n",
    "        result = []\n",
    "        decoder = 'greedy'\n",
    "        character = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\]^_`|~กขฃคฅฆงจฉชซฌญฎฏฐฑฒณดตถทธนบปผฝพฟภมยรฤลฦวศษสหฬอฮฯะัาำิีึืุูเแโใไๅๆ็่้๊๋์ํ๐๑๒๓๔๕๖๗๘๙–‘ ’'\n",
    "        separator_list = {}\n",
    "        dict_list = {'en': 'd:\\\\Intern\\\\EasyOCR\\\\easyocr\\\\dict\\\\en.txt', 'th': 'd:\\\\Intern\\\\EasyOCR\\\\easyocr\\\\dict\\\\th.txt'}\n",
    "        converter = CTCLabelConverter(character, separator_list, dict_list)\n",
    "        if decoder == 'greedy':\n",
    "            # Select max probabilty (greedy decoding) then decode index to character\n",
    "            _, preds_index = preds_prob.max(2)\n",
    "            preds_index = preds_index.view(-1)\n",
    "            preds_str = converter.decode_greedy(preds_index.data.cpu().detach().numpy(), preds_size.data)\n",
    "        elif decoder == 'beamsearch':\n",
    "            k = preds_prob.cpu().detach().numpy()\n",
    "            preds_str = converter.decode_beamsearch(k, beamWidth=5)\n",
    "        elif decoder == 'wordbeamsearch':\n",
    "            k = preds_prob.cpu().detach().numpy()\n",
    "            preds_str = converter.decode_wordbeamsearch(k, beamWidth=5)\n",
    "\n",
    "        preds_prob = preds_prob.cpu().detach().numpy()\n",
    "        values = preds_prob.max(axis=2)\n",
    "        indices = preds_prob.argmax(axis=2)\n",
    "        preds_max_prob = []\n",
    "        for v,i in zip(values, indices):\n",
    "            max_probs = v[i!=0]\n",
    "            if len(max_probs)>0:\n",
    "                preds_max_prob.append(max_probs)\n",
    "            else:\n",
    "                preds_max_prob.append(np.array([0]))\n",
    "\n",
    "        for pred, pred_max_prob in zip(preds_str, preds_max_prob):\n",
    "            confidence_score = custom_mean(pred_max_prob)\n",
    "            result.append([pred, confidence_score])\n",
    "        try:\n",
    "            pred_list.append(result)\n",
    "        except:\n",
    "            pred_list.append('')\n",
    "\n",
    "    label_img = label[i][1]\n",
    "    name_img.append(label[i][0])\n",
    "    gt.append(label_img)\n",
    "\n",
    "end = time.time()\n",
    "print(f'time: {end - start}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = name_img.copy()\n",
    "output, false_list = eval_ocr_metric(pred_list, gt, tmp)\n",
    "# output, false_list = eval_ocr_metric(pred, gt, name_img)\n",
    "for k,v in output.items():\n",
    "    print(f'{k}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepaths = glob.glob(os.path.join('trainer/all_data/testing/testing','*.jpg'))\n",
    "# # filepaths = glob.glob(os.path.join('trainer/all_data/training/training','*.jpg'))[:1000]\n",
    "# filepaths = sorted(filepaths, key= lambda x: int(re.split(r'[/\\\\.]',x)[-2]))\n",
    "# label_path = 'trainer/all_data/testing/testing/label.txt'\n",
    "# with open(label_path, encoding='utf8') as file:\n",
    "#     label = np.loadtxt(file,dtype=str)\n",
    "# start = time.time()\n",
    "# # filepaths = filepaths[:10]\n",
    "# pred = []\n",
    "# gt = [] # ground truth\n",
    "# name_img = []\n",
    "# for i in tqdm(range(len(filepaths))):\n",
    "#     label_img = label[i][1]\n",
    "#     name_img.append(label[i][0])\n",
    "#     result = reader.recognize(filepaths[i],detail=0)\n",
    "#     try:\n",
    "#         pred.append(result)\n",
    "#     except:\n",
    "#         pred.append('')\n",
    "#     gt.append(label_img)\n",
    "# end = time.time()\n",
    "# print(f'time: {end - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = name_img.copy()\n",
    "# output, false_list = eval_ocr_metric(pred, gt, tmp)\n",
    "# # output, false_list = eval_ocr_metric(pred, gt, name_img)\n",
    "# for k,v in output.items():\n",
    "#     print(f'{k}: {v}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52122cbc9699272b0fccdd336f01da0a8fa089bdcccefc5e05c668ba6be669f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
