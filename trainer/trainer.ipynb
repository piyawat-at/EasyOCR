{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import yaml\n",
    "from utils import AttrDict\n",
    "import pandas as pd\n",
    "from train import train\n",
    "from data_prepare import create_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# !pip install natsort\n",
    "# !pip install torchvision\n",
    "# !pip install pyyaml\n",
    "# !pip install pandas\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T04:19:23.885144Z",
     "start_time": "2021-07-23T04:19:23.880564Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cudnn.benchmark = True\n",
    "cudnn.deterministic = False\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = os.path.join('all_data','training','training')\n",
    "# create_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T04:19:24.119144Z",
     "start_time": "2021-07-23T04:19:24.112032Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_config(file_path):\n",
    "    with open(file_path, 'r', encoding=\"utf8\") as stream:\n",
    "        opt = yaml.safe_load(stream)\n",
    "    opt = AttrDict(opt)\n",
    "    if opt.lang_char == 'None':\n",
    "        characters = ''\n",
    "        for data in opt['select_data'].split('-'):\n",
    "            csv_path = os.path.join(opt['train_data'], 'training' ,data, 'labels.csv')\n",
    "            df = pd.read_csv(csv_path, sep='^([^,]+),', engine='python', usecols=['filename', 'words'], keep_default_na=False)\n",
    "            all_char = ''.join(df['words'])\n",
    "            characters += ''.join(set(all_char))\n",
    "        characters = sorted(set(characters))\n",
    "        opt.character= ''.join(characters)\n",
    "        print(f'experiment: {opt.experiment_name}')\n",
    "        print(f'char in train_set:{opt.character} class:{len(opt.character)}')\n",
    "        with open('test.ymal','w') as stream:\n",
    "            yaml.dump(opt, stream)\n",
    "    else:\n",
    "        characters = ''\n",
    "        for data in opt['select_data'].split('-'):\n",
    "            csv_path = os.path.join(opt['train_data'], 'training' ,data, 'labels.csv')\n",
    "            df = pd.read_csv(csv_path, sep='^([^,]+),', engine='python', usecols=['filename', 'words'], keep_default_na=False)\n",
    "            all_char = ''.join(df['words'])\n",
    "            characters += ''.join(set(all_char))\n",
    "        characters = sorted(set(characters))\n",
    "        opt.character= ''.join(characters)\n",
    "        print(f'experiment: {opt.experiment_name}')\n",
    "        print(f'char in train_set:{opt.character} class:{len(opt.character)}')\n",
    "        opt.character = opt.lang_char\n",
    "        print(f'char in config:{opt.character} class:{len(opt.character)}')\n",
    "    os.makedirs(f'./saved_models/{opt.experiment_name}', exist_ok=True)\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "albe to config in config_files/*.yaml\n",
    "check list: \\\n",
    "&emsp;lang_char \\\n",
    "&emsp;experiment_name \\\n",
    "&emsp;num_iter \\\n",
    "&emsp;saved_model \\\n",
    "&emsp;lr \\\n",
    "&emsp;model architecture \\\n",
    "&emsp;check log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T04:49:07.045060Z",
     "start_time": "2021-07-23T04:20:15.050992Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment: crnn_hippo_1M\n",
      "char in train_set: !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz|~×àáéĄกขฃคฅฆงจฉชซฌญฎฏฐฑฒณดตถทธนบปผฝพฟภมยรฤลฦวศษสหฬอฮฯะัาำิีึืฺุู฿เแโใไๅๆ็่้๊๋์ํ๎๐๑๒๓๔๕๖๗๘๙–—“”•✓ class:188\n",
      "Filtering the images containing characters which are not in opt.character\n",
      "Filtering the images whose label is longer than opt.batch_max_length\n",
      "--------------------------------------------------------------------------------\n",
      "dataset_root: all_data/hippo\n",
      "opt.select_data: ['700k_w_VQOS', '300k_r_Y92B', 'google_ocr_train_images']\n",
      "opt.batch_ratio: ['1', '1', '1']\n",
      "--------------------------------------------------------------------------------\n",
      "dataset_log dataset_root:    all_data/hippo\t dataset: 700k_w_VQOS\n",
      "all_data/hippo/training/700k_w_VQOS\n",
      "sub-directory:\t/training/700k_w_VQOS\t num samples: 699991\n",
      "num total samples of 700k_w_VQOS: 699991 x 1.0 (total_data_usage_ratio) = 699991\n",
      "num samples of 700k_w_VQOS per batch: 8 x 1.0 (batch_ratio) = 8\n",
      "--------------------------------------------------------------------------------\n",
      "dataset_log dataset_root:    all_data/hippo\t dataset: 300k_r_Y92B\n",
      "all_data/hippo/training/300k_r_Y92B\n",
      "sub-directory:\t/training/300k_r_Y92B\t num samples: 299997\n",
      "num total samples of 300k_r_Y92B: 299997 x 1.0 (total_data_usage_ratio) = 299997\n",
      "num samples of 300k_r_Y92B per batch: 8 x 1.0 (batch_ratio) = 8\n",
      "--------------------------------------------------------------------------------\n",
      "dataset_log dataset_root:    all_data/hippo\t dataset: google_ocr_train_images\n",
      "all_data/hippo/training/google_ocr_train_images\n",
      "sub-directory:\t/training/google_ocr_train_images\t num samples: 14228\n",
      "num total samples of google_ocr_train_images: 14228 x 1.0 (total_data_usage_ratio) = 14228\n",
      "num samples of google_ocr_train_images per batch: 8 x 1.0 (batch_ratio) = 8\n",
      "--------------------------------------------------------------------------------\n",
      "Total_batch_size: 8+8+8 = 24\n",
      "--------------------------------------------------------------------------------\n",
      "dataset_log dataset_root:    all_data/validation\t dataset: /\n",
      "all_data/validation/validation\n",
      "sub-directory:\t/validation\t num samples: 2987\n",
      "--------------------------------------------------------------------------------\n",
      "No Transformation module specified\n",
      "model input parameters 64 600 20 1 256 256 189 100000 None VGG BiLSTM CTC\n",
      "loading pretrained model from saved_models/crnn_hippo_1M/iter_80000.pth\n",
      "Modules, Parameters\n",
      "module.FeatureExtraction.ConvNet.0.weight 288\n",
      "module.FeatureExtraction.ConvNet.0.bias 32\n",
      "module.FeatureExtraction.ConvNet.3.weight 18432\n",
      "module.FeatureExtraction.ConvNet.3.bias 64\n",
      "module.FeatureExtraction.ConvNet.6.weight 73728\n",
      "module.FeatureExtraction.ConvNet.6.bias 128\n",
      "module.FeatureExtraction.ConvNet.8.weight 147456\n",
      "module.FeatureExtraction.ConvNet.8.bias 128\n",
      "module.FeatureExtraction.ConvNet.11.weight 294912\n",
      "module.FeatureExtraction.ConvNet.12.weight 256\n",
      "module.FeatureExtraction.ConvNet.12.bias 256\n",
      "module.FeatureExtraction.ConvNet.14.weight 589824\n",
      "module.FeatureExtraction.ConvNet.15.weight 256\n",
      "module.FeatureExtraction.ConvNet.15.bias 256\n",
      "module.FeatureExtraction.ConvNet.18.weight 262144\n",
      "module.FeatureExtraction.ConvNet.18.bias 256\n",
      "module.SequenceModeling.0.rnn.weight_ih_l0 262144\n",
      "module.SequenceModeling.0.rnn.weight_hh_l0 262144\n",
      "module.SequenceModeling.0.rnn.bias_ih_l0 1024\n",
      "module.SequenceModeling.0.rnn.bias_hh_l0 1024\n",
      "module.SequenceModeling.0.rnn.weight_ih_l0_reverse 262144\n",
      "module.SequenceModeling.0.rnn.weight_hh_l0_reverse 262144\n",
      "module.SequenceModeling.0.rnn.bias_ih_l0_reverse 1024\n",
      "module.SequenceModeling.0.rnn.bias_hh_l0_reverse 1024\n",
      "module.SequenceModeling.0.linear.weight 131072\n",
      "module.SequenceModeling.0.linear.bias 256\n",
      "module.SequenceModeling.1.rnn.weight_ih_l0 262144\n",
      "module.SequenceModeling.1.rnn.weight_hh_l0 262144\n",
      "module.SequenceModeling.1.rnn.bias_ih_l0 1024\n",
      "module.SequenceModeling.1.rnn.bias_hh_l0 1024\n",
      "module.SequenceModeling.1.rnn.weight_ih_l0_reverse 262144\n",
      "module.SequenceModeling.1.rnn.weight_hh_l0_reverse 262144\n",
      "module.SequenceModeling.1.rnn.bias_ih_l0_reverse 1024\n",
      "module.SequenceModeling.1.rnn.bias_hh_l0_reverse 1024\n",
      "module.SequenceModeling.1.linear.weight 131072\n",
      "module.SequenceModeling.1.linear.bias 256\n",
      "module.Prediction.weight 48384\n",
      "module.Prediction.bias 189\n",
      "Total Trainable Params: 3804989\n",
      "Trainable params num :  3804989\n",
      "Optimizer:\n",
      "Adadelta (\n",
      "Parameter Group 0\n",
      "    eps: 1e-08\n",
      "    lr: 0.25\n",
      "    rho: 0.95\n",
      "    weight_decay: 0\n",
      ")\n",
      "------------ Options -------------\n",
      "lang_char: None\n",
      "experiment_name: crnn_hippo_1M\n",
      "train_data: all_data/hippo\n",
      "valid_data: all_data/validation\n",
      "manualSeed: 1111\n",
      "workers: 6\n",
      "batch_size: 24\n",
      "num_iter: 100000\n",
      "valInterval: 1000000\n",
      "saved_model: saved_models/crnn_hippo_1M/iter_80000.pth\n",
      "FT: False\n",
      "optim: False\n",
      "lr: 0.25\n",
      "beta1: 0.9\n",
      "rho: 0.95\n",
      "eps: 1e-08\n",
      "grad_clip: 5\n",
      "select_data: ['700k_w_VQOS', '300k_r_Y92B', 'google_ocr_train_images']\n",
      "batch_ratio: ['1', '1', '1']\n",
      "total_data_usage_ratio: 1.0\n",
      "batch_max_length: 100000\n",
      "imgH: 64\n",
      "imgW: 600\n",
      "rgb: False\n",
      "contrast_adjust: 0.0\n",
      "sensitive: True\n",
      "PAD: True\n",
      "data_filtering_off: False\n",
      "Transformation: None\n",
      "FeatureExtraction: VGG\n",
      "SequenceModeling: BiLSTM\n",
      "Prediction: CTC\n",
      "num_fiducial: 20\n",
      "input_channel: 1\n",
      "output_channel: 256\n",
      "hidden_size: 256\n",
      "decode: greedy\n",
      "new_prediction: False\n",
      "freeze_FeatureFxtraction: False\n",
      "freeze_SequenceModeling: False\n",
      "character:  !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz|~×àáéĄกขฃคฅฆงจฉชซฌญฎฏฐฑฒณดตถทธนบปผฝพฟภมยรฤลฦวศษสหฬอฮฯะัาำิีึืฺุู฿เแโใไๅๆ็่้๊๋์ํ๎๐๑๒๓๔๕๖๗๘๙–—“”•✓\n",
      "num_class: 189\n",
      "---------------------------------------\n",
      "\n",
      "continue to train, start_iter: 80000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.132:   4%|▎         | 702/20000 [02:24<1:02:17,  5.16it/s]"
     ]
    }
   ],
   "source": [
    "opt = get_config(\"config_files/crnn_1M.yaml\")\n",
    "train(opt, amp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_path = [ 'saved_models/rosetta_hippo_300k_v1/log_loss_train.txt'\n",
    "]\n",
    "loss_list = []\n",
    "for path in label_path:\n",
    "    with open(path, encoding='utf8') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            if not line.startswith(('experiment_name')) and not  not line.strip():\n",
    "                loss_list.append(float(line.split(' ')[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "per iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "print(len(loss_list))\n",
    "start_row = 15000\n",
    "start = int(start_row /10)\n",
    "# plt.plot(np.arange(len(loss_list[start:]))*10+start_row, loss_list[start:])\n",
    "plt.figure(figsize=(10, 10), dpi=80)\n",
    "plt.plot(loss_list[start:])\n",
    "\n",
    "plt.title(f'Training loss afer {start_row} iterations')\n",
    "plt.title(f'Training loss')\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "print(len(loss_list))\n",
    "batch_size = 32\n",
    "iteration = len(loss_list)*10\n",
    "n_sample = 99331\n",
    "epoch = (iteration*batch_size) // n_sample\n",
    "epoch_avg = np.array([])\n",
    "for i in range(epoch):\n",
    "    epoch_avg = np.append(epoch_avg, np.mean(loss_list[int(i*n_sample/320):(i+1)*int(i*n_sample/320)]))\n",
    "plt.figure(figsize=(10, 10), dpi=80)\n",
    "start_row = 0\n",
    "start = int(start_row /10)\n",
    "# plt.plot(np.arange(len(loss_list[start:]))*10+start_row, loss_list[start:])\n",
    "plt.plot(epoch_avg[:5])\n",
    "# plt.title(f'Training loss afer {start_row} iterations')\n",
    "plt.title(f'Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
